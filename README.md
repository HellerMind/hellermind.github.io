# WebLLM - Local AI Chat

A beautiful, modern web interface for running Qwen3-0.6B locally in your browser. Experience real-time streaming responses with a stunning UI design.

## 🌟 Features

- **Local AI Processing**: Runs completely in your browser - no server required
- **Real-time Streaming**: Responses appear word-by-word as they're generated
- **Beautiful UI**: Modern, responsive design with smooth animations
- **Qwen3-0.6B Model**: Fast, efficient language model optimized for browser use
- **Thinking Indicators**: Visual feedback during response generation
- **Mobile Responsive**: Works perfectly on all devices
- **Offline Capable**: Once loaded, works without internet connection

## 🚀 Getting Started

1. **Open the Application**
   - Simply open `index.html` in your web browser
   - No installation or setup required

2. **Wait for Model Loading**
   - The model will download and load automatically
   - Progress is shown with a beautiful loading screen
   - This may take a few minutes on first load

3. **Start Chatting**
   - Type your message in the input field
   - Press Enter or click Send
   - Watch responses appear in real-time

## 🎨 Design Features

- **Dark Theme**: Easy on the eyes with beautiful gradients
- **Smooth Animations**: Fluid transitions and hover effects
- **Loading Screen**: Elegant progress indicator with status updates
- **Message Bubbles**: Clear distinction between user and AI messages
- **Auto-resize Input**: Textarea grows with your message
- **Scroll to Bottom**: Automatic scrolling to latest messages

## 🔧 Technical Details

- **Model**: Qwen3-0.6B (quantized for browser performance)
- **Framework**: Vanilla JavaScript with ES6 modules
- **Styling**: Modern CSS with CSS custom properties
- **Loading**: Uses Transformers.js for model loading
- **Streaming**: Real-time token streaming for natural conversation flow

## 📱 Browser Compatibility

- Chrome/Chromium (recommended)
- Firefox
- Safari
- Edge

**Note**: WebGPU support is recommended for optimal performance.

## 🎯 Usage Tips

1. **First Load**: The model download may take several minutes depending on your internet connection
2. **Responses**: The AI will respond with streaming text, appearing word by word
3. **Thinking**: You'll see "Thinking..." indicators while the AI processes your request
4. **Long Messages**: The textarea automatically resizes to accommodate longer messages
5. **Mobile**: Works great on mobile devices with touch-friendly interface

## 🛠️ Customization

The application is built with modern web standards and can be easily customized:

- **Colors**: Modify CSS custom properties in `:root`
- **Model**: Change the model in the JavaScript code
- **Styling**: All styles are in the `<style>` section
- **Functionality**: Modular JavaScript classes for easy modification

## 🔒 Privacy

- **100% Local**: All processing happens in your browser
- **No Data Sent**: No messages are sent to external servers
- **Offline Capable**: Works without internet after initial model download

## 🎨 UI Components

- **Loading Screen**: Animated spinner with progress bar
- **Chat Interface**: Clean message bubbles with smooth animations
- **Input Area**: Auto-resizing textarea with send button
- **Status Indicators**: Real-time model status and connection indicators
- **Responsive Design**: Adapts beautifully to all screen sizes

## 🚀 Performance

- **Optimized Model**: Quantized for faster browser execution
- **Efficient Loading**: Progressive loading with progress indicators
- **Smooth Animations**: Hardware-accelerated CSS animations
- **Memory Efficient**: Minimal memory footprint for browser compatibility

## 📄 License

This project is open source and available under the MIT License.

---

**Enjoy chatting with your local AI! 🤖✨**